{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../../')\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import TimeSeriesDataset, ListDataset\n",
    "from src.data.functions import torch_ffill\n",
    "from src.features.derived_features import shock_index, partial_sofa\n",
    "from src.model.model_selection import stratified_kfold_cv\n",
    "from src.model.nets import RNN\n",
    "from src.model.optimizer import optimize_utility_threshold, compute_utility_from_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Prediction Example\n",
    "This notebook will explain how to setup the data to use an RNN for prediction. It follows closely the example in `src/model/examples/train_rnn.py` but includes further explanation regarding each step. \n",
    "\n",
    "This is designed to show how one can integrate a sequential deep-learning model into the current framework. The general procedure will involve the same basic steps, but different features will be derived and chosen by the user, and the model will most likely be replaced with something else along with an adapted training procedure. Evaluation against the utility score is tricky (it is overviewed in part 10.) and so I suggest that if you wish to use this as an evaluation method you copy and paste the evaluation part and ensure your inputs are of the required form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a GPU device to train with if one is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the raw data from save. This includes the TimeSeriesDataset format of the training data, and the utility score labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "dataset = TimeSeriesDataset().load(DATA_DIR + '/raw/data.tsd')\n",
    "labels = torch.Tensor(load_pickle(DATA_DIR + '/processed/labels/utility_scores.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward fill the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data = torch_ffill(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expert Features\n",
    "Compute some 'expert knowledge' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['PartialSOFA'] = partial_sofa(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time-Series Features\n",
    "Get any additional features, such as signatures, mins, maxs, etc. over some rolling window that you want to put into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['MaxShockIndex'] = RollingStatistic(statistic='max', window_length=5).transform(dataset['SBP'])\n",
    "# dataset['MinHR'] = RollingStatistic(statistic='min', window_length=8).transform(dataset['HR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We comment these out as we will not actually use any in this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choose a Subset of the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variables change *very* infrequently and the feature set is very large. This is not great for an RNN model, so we choose to subset only those features that are frequently changing. \n",
    "\n",
    "For this example we look at the vitals signs: ['DBP', 'SBP', 'Temp', 'HR', 'MAP', 'Resp'], the SOFA score ['PartialSofa'], and time ['ICULOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.subset(['DBP', 'SBP', 'Temp', 'HR', 'MAP', 'PartialSOFA', 'ICULOS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fill Missing Values\n",
    "RNNs cannot handle missing values, and we typically have a lot of nans (though this is not so much the case with the frequently changing features). Anyway, here we simply fill nans with the zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data[torch.isnan(dataset.data)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Setup DataLoaders\n",
    "Now we will setup the methods to feed the data into the RNN model. To start with, convert the data from the filled tensor format onto a list of variable length tensors. `data_list[0]` will correspond to a tensor of values for a given patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = dataset.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to setup the cross-validation procedure. This is a little tricky and will be improved. The idea is that we want the same number of 'eventual sepsis' cases in each fold, and no patient being in more than one fold. The code below is simply ensuring this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the id-indexed CV fold. We need both patient indexes and time index.\n",
    "cv, id_cv = stratified_kfold_cv(dataset, labels, n_splits=5, return_as_list=True, seed=1)\n",
    "train_idxs, test_idxs = cv[0]\n",
    "train_id_idxs, test_id_idxs = id_cv[0]\n",
    "\n",
    "# Make train and test data\n",
    "train_data = [data_list[i].to(device) for i in train_id_idxs]\n",
    "train_labels = [labels[i].to(device) for i in train_idxs]\n",
    "test_data = [data_list[i].to(device) for i in test_id_idxs]\n",
    "test_labels = [labels[i].to(device) for i in test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put this into a PyTorch dataloading format. We make a custom `ListDataset` class such that indexing returns one of the entries from the `data_list` along with the corresponding labels for each time-point of that patient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_ds = ListDataset(train_data, train_labels)\n",
    "test_ds = ListDataset(test_data, test_labels)\n",
    "\n",
    "# Dataloaders. We use a batch size of 1 as we have lists not tensors.\n",
    "train_dl = DataLoader(train_ds, batch_size=1)\n",
    "test_dl = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Setup Model\n",
    "Now the data is in a standard format for training, we setup the RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(7, 10, batch_first=True)\n",
       "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model setup\n",
    "model = RNN(in_channels=dataset.size(2), hidden_channels=10, out_channels=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train Model\n",
    "Now run a standard training loop monitoring losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:00, 198.77it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 1\n",
    "print_freq = 1\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    train_losses = []\n",
    "    for i, batch in tqdm(enumerate(train_dl)):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, true = batch\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1), true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    if epoch % print_freq == 0:\n",
    "        train_loss = np.mean(train_losses)\n",
    "        print(\"Epoch: {:.3f}  Average training loss: {:.3f}\".format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Generate Predictions\n",
    "Now simply loop over the training and testing sets to generate the predictions for each, and concatenate everything into a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "train_preds, test_preds = [], []\n",
    "with torch.no_grad():\n",
    "    # Predict train\n",
    "    for batch in train_data:\n",
    "        train_preds.append(model(batch.unsqueeze(0)).view(-1))\n",
    "\n",
    "    # Predict test\n",
    "    for batch in test_data:\n",
    "        test_preds.append(model(batch.unsqueeze(0)).view(-1))\n",
    "\n",
    "# Concat\n",
    "train_preds = torch.cat(train_preds).view(-1).detach()\n",
    "test_preds = torch.cat(test_preds).view(-1).detach()\n",
    "train_labels = torch.cat(train_labels).view(-1).detach()\n",
    "test_labels = torch.cat(test_labels).view(-1).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evaluate the Performance of the Models \n",
    "Finally evaluate model performance. The easiest thing to do is evaluate the losses, but this doesn't tell you much. To evaluate properly we need to find what our score on the challenge utility function is. We give a description of what you need to do and how that is achieved below. \n",
    "\n",
    "**Utility function evaluation**\n",
    "We have optimised our model to estimate the utility score for each timepoint for each patient. However, predictions must be given as a 0-1 binary classification as to whether a person has sepsis. Thus we need to define a threshold for which if our prediction exceeds it, we will define that patient as having sepsis. We find this threshold via a gradient free optimization procedure on the training set. The outline is as follows: \n",
    "\n",
    "1. Start with an initial guess (threshold = 0) and evaluate the training utility with that threshold.\n",
    "2. Search around this guess using some sensible optimisation procedure, each time evaluating the utility given that new tested threshold. \n",
    "3. Continue until convergence. \n",
    "\n",
    "This is what is going on internally in these functions below. One thing that should be noted is that evaluating the utility from scratch for each patient takes an extremely long time, and given that we normally have around 200 iterations until convergence, it is on the order of hours before this optimization scheme converges. What we do instead is compute one time the utility one would achieve for predicting 0 at each time-point in the dataset, and predicting 1 at each point in the dataset. This is stored on disk in `data/processed/labels/full_scores.pickle`. Now when we make new predictions, provided we save the indexes those predictions correspond to in the full dataset, we can extract the score achieved (either by pulling from the 0 or 1 column of the saved scores, depending on what our prediction was at that point) from the pre-computed saved scores. Then simply sum up and normalise. \n",
    "\n",
    "This is just meant to explain why this evaluation is a little tricky, and the evaluation below is a little strange, and requires the cross validation indexes to be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute losses\n",
    "train_loss = loss_fn(train_labels, train_preds)\n",
    "test_loss = loss_fn(test_labels, test_preds)\n",
    "print('Train loss: {:.3f}'.format(train_loss))\n",
    "print('Test loss: {:.3f}'.format(test_loss))\n",
    "\n",
    "# Compute the score on the utility function\n",
    "train_idxs, test_idxs = torch.cat(train_idxs), torch.cat(test_idxs)\n",
    "tfm_np = lambda x: x.cpu().numpy()\n",
    "train_preds, test_preds = tfm_np(train_preds), tfm_np(test_preds)\n",
    "thresh = optimize_utility_threshold(train_preds, idxs=train_idxs)\n",
    "train_utility = compute_utility_from_indexes(train_preds, thresh, idxs=train_idxs)\n",
    "test_utility = compute_utility_from_indexes(test_preds, thresh, idxs=test_idxs)\n",
    "print('Train utility score: {:.3f}'.format(train_utility))\n",
    "print('Test utility score: {:.3f}'.format(test_utility))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
